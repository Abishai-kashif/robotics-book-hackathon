# Data Model: Backend Tech Stack Refactor

**Feature**: 007-backend-tech-stack-refactor
**Date**: 2025-12-28
**Purpose**: Define data structures, relationships, and state management for refactored backend

## Overview

This refactor maintains existing data models while introducing new agent-related structures and removing local Qdrant fallback logic. Key changes: add agent configuration models, update Qdrant client initialization, preserve conversation and embedding schemas.

## Core Entities

### 1. Environment Configuration

**Purpose**: Centralize and validate all environment variables for cloud services

**Entity**: `EnvironmentConfig` (New)

**Fields**:
- `qdrant_cluster_endpoint`: str - Full URL to Qdrant Cloud cluster (required)
- `qdrant_api_key`: str - Authentication key for Qdrant Cloud (required, sensitive)
- `gemini_base_url`: str | None - Gemini API endpoint (optional, for custom client)
- `gemini_api_key`: str | None - Gemini authentication key (optional, sensitive)
- `openai_api_key`: str | None - OpenAI authentication key (optional, sensitive)
- `model_name`: str - LLM model identifier (default: "gemini-2.5-flash-preview")
- `embedding_model`: str - sentence-transformers model name (default: "all-MiniLM-L6-v2")
- `python_version`: str - Python runtime version (informational)

**Validation Rules**:
- At least one of (`gemini_api_key` + `gemini_base_url`) OR `openai_api_key` must be provided
- `qdrant_cluster_endpoint` must start with "https://" (TLS required)
- `qdrant_api_key` must be non-empty string
- Fail fast on startup if validation fails

**Implementation Location**: `backend/src/config/env.py`

**State**: Loaded once at application startup, immutable during runtime

### 2. Vector Embedding

**Purpose**: Represent textbook content as numerical vectors for semantic search

**Entity**: `VectorEmbedding` (Existing, unchanged)

**Fields**:
- `id`: str - Unique identifier (MD5 hash of content)
- `vector`: list[float] - 384-dimensional embedding (sentence-transformers output)
- `payload`: dict - Metadata including:
  - `title`: str - Document/section title
  - `content`: str - Original text chunk
  - `page`: str | None - Source page identifier
  - `section_header`: str | None - Hierarchical section location
  - `timestamp`: datetime - Indexing timestamp

**Validation Rules**:
- `vector` must be exactly 384 dimensions (matches embedding model)
- `id` must be unique within collection
- `content` length: 100-2000 characters (chunking boundaries)

**Relationships**:
- Stored in Qdrant Cloud collection: `textbook_content`
- Retrieved by `QdrantService` during search operations
- Generated by `TextSplitter` and embedding model pipeline

**State Transitions**: Immutable once indexed (updates require re-indexing with new ID)

### 3. Textbook Content

**Purpose**: Structured representation of textbook documents for indexing

**Entity**: `TextbookContent` (Existing, unchanged)

**Fields**:
- `content_id`: str - Unique identifier (MD5 hash)
- `title`: str - Document title from frontmatter
- `content`: str - Text chunk (1000 chars, 200 char overlap)
- `metadata`: dict - Additional properties:
  - `source_file`: str - Original markdown file path
  - `chunk_index`: int - Position in document (0-indexed)
  - `total_chunks`: int - Total chunks in document

**Validation Rules**:
- `title` required, 1-200 characters
- `content` required, 100-2000 characters
- `chunk_index` >= 0, `total_chunks` >= 1
- `chunk_index` < `total_chunks`

**Lifecycle**:
1. Created: Markdown file parsed by `index_content.py`
2. Chunked: `MarkdownTextSplitter` splits by headers/paragraphs
3. Embedded: sentence-transformers generates vector
4. Stored: Uploaded to Qdrant Cloud with embedding

### 4. Agent Configuration

**Purpose**: Define LLM client settings for OpenAI Agents SDK

**Entity**: `AgentConfig` (New)

**Fields**:
- `provider`: Literal["openai", "gemini"] - LLM provider selection
- `model_name`: str - Model identifier
- `base_url`: str | None - Custom API endpoint (for Gemini)
- `api_key`: str - Authentication credential (sensitive)
- `temperature`: float - Sampling temperature (default: 0.7)
- `max_tokens`: int - Maximum response length (default: 1000)

**Validation Rules**:
- `provider == "gemini"` requires `base_url` and `api_key`
- `provider == "openai"` requires `api_key` only
- `temperature` range: 0.0-1.0
- `max_tokens` range: 1-4096

**Implementation Location**: `backend/src/agents/models.py`

**Usage**: Factory functions create `AsyncOpenAI` clients and `OpenAIChatCompletionsModel` instances

### 5. Conversation Context

**Purpose**: Maintain multi-turn conversation state for agent interactions

**Entity**: `ConversationSession` (Existing, refactored to use SDK session management)

**Fields** (SDK-managed, not explicitly modeled):
- `session_id`: str - Unique session identifier (UUID)
- `history`: list[Message] - Conversation messages with roles (user/assistant/system)
- `created_at`: datetime - Session creation timestamp
- `last_activity`: datetime - Most recent message timestamp

**SDK Integration**:
- OpenAI Agents SDK handles session management internally
- Sessions persisted in-memory during agent execution
- Conversation history automatically maintained across turns

**Deprecation Note**: Custom `ConversationSession` model in `backend/src/models/embedding.py` will be removed; SDK's built-in session management replaces it

### 6. User Query

**Purpose**: Represent incoming user questions for processing

**Entity**: `UserQuery` (Existing, unchanged)

**Fields**:
- `query_id`: str - Unique identifier (UUID)
- `timestamp`: datetime - Query creation time
- `content`: str - User's question (2-1000 characters)
- `source_page`: str | None - Current page context (optional)
- `session_id`: str | None - Conversation session reference (optional)
- `user_context`: dict | None - Additional metadata (optional)

**Validation Rules**:
- `content` required, 2-1000 characters
- `content` must not contain malicious scripts (input validation middleware)
- `query_id` auto-generated if not provided

**Lifecycle**:
1. Received: API endpoint `/api/v1/chat` receives POST request
2. Validated: Middleware checks content length and safety
3. Embedded: Converted to vector for Qdrant search
4. Processed: Agent generates response with retrieved context
5. Returned: `RAGResponse` sent to client

### 7. RAG Response

**Purpose**: Structured response from agent with sources and confidence

**Entity**: `RAGResponse` (Existing, unchanged)

**Fields**:
- `answer`: str - Generated response text from agent
- `sources`: list[dict] - Citations with:
  - `title`: str - Source document title
  - `page`: str | None - Page reference
  - `relevance_score`: float - Similarity score (0-1)
- `confidence_score`: float - Aggregate confidence (0-1, based on top source relevance)
- `processing_time_ms`: float - Query latency metric

**Validation Rules**:
- `answer` non-empty string
- `sources` list may be empty (no relevant content found)
- `confidence_score` range: 0.0-1.0
- `processing_time_ms` >= 0

**Calculation Logic**:
- `confidence_score` = max(`relevance_score` from top 3 sources)
- `relevance_score` = cosine similarity between query and document embeddings

## Relationships

### Entity Relationship Diagram

```
┌─────────────────────┐
│ EnvironmentConfig   │
│  (startup singleton)│
└──────────┬──────────┘
           │ configures
           ▼
┌─────────────────────┐       ┌──────────────────┐
│  QdrantClient       │◄──────│  VectorEmbedding │
│  (cloud connection) │       │   (384-dim)      │
└──────────┬──────────┘       └────────┬─────────┘
           │ searches                   │ generated from
           ▼                            │
┌─────────────────────┐                │
│  UserQuery          │                │
│  (incoming request) │                │
└──────────┬──────────┘                │
           │ embedded & searched        │
           │                            │
           ▼                            │
┌─────────────────────┐       ┌────────▼─────────┐
│  Agent (SDK)        │◄──────│ TextbookContent  │
│  (conversation AI)  │       │  (chunked docs)  │
└──────────┬──────────┘       └──────────────────┘
           │ generates
           ▼
┌─────────────────────┐
│  RAGResponse        │
│  (answer + sources) │
└─────────────────────┘
```

### Data Flow

1. **Startup**: `EnvironmentConfig` validates and loads cloud credentials
2. **Indexing**: `TextbookContent` → chunked → embedded → `VectorEmbedding` → Qdrant Cloud
3. **Query Processing**:
   - `UserQuery` received via API
   - Embedded using sentence-transformers
   - Searched in Qdrant Cloud (returns top-k `VectorEmbedding` matches)
   - Relevant content passed to `Agent` as context
   - Agent generates `RAGResponse` with citations

## State Management

### Application Lifecycle States

1. **Initialization**:
   - Load `EnvironmentConfig`
   - Validate all required credentials
   - Initialize `QdrantClient` with cloud endpoint
   - Verify Qdrant Cloud connectivity (health check)
   - Load agent models (lazy initialization for efficiency)

2. **Runtime**:
   - Accept `UserQuery` via API
   - Maintain agent session state (SDK-managed)
   - Cache embedding model in memory (singleton)
   - Log performance metrics (query latency, error rates)

3. **Shutdown**:
   - Graceful cleanup of Qdrant client connections
   - Flush metrics and logs
   - No persistent state to save (stateless backend)

### Error States

| Error Condition | Handling Strategy | Recovery |
|----------------|-------------------|----------|
| Missing env vars | Fail fast on startup | Fix .env, restart |
| Qdrant Cloud connection failure | Retry with exponential backoff (3 attempts) | Return 503 to client, alert ops |
| Invalid API key | Log error, return 401 | Rotate key, update env vars |
| Agent timeout (>30s) | Cancel request, return partial response | Client can retry |
| Embedding generation error | Skip document, log warning | Continue with remaining docs |
| Rate limit exceeded | Exponential backoff, queue request | Retry after delay |

## Migration Impact

### Data Preservation

- **Embeddings**: Existing vectors compatible with Qdrant Cloud (384-dim, COSINE)
- **Schema**: Collection structure unchanged (id, vector, payload)
- **API**: Request/response models unchanged (`UserQuery`, `RAGResponse`)

### Data Migration Strategy

**Recommended**: Fresh indexing (run `scripts/index_content.py` against Qdrant Cloud)

**Alternative**: Export/import using Qdrant snapshot API (if preserving exact IDs needed)

**Steps**:
1. Provision Qdrant Cloud cluster
2. Create `textbook_content` collection with 384-dim COSINE vectors
3. Run indexing script with cloud credentials
4. Verify sample queries return expected results
5. Switch API backend to cloud configuration
6. Decommission local Docker Qdrant

### Rollback Plan

- Keep local Docker Compose configuration in git (commented out)
- Maintain separate staging Qdrant Cloud cluster for testing
- Can revert env vars to local Docker if cloud issues arise
- No data loss: re-run indexing script against local cluster

## Validation & Testing

### Unit Tests

- `test_env_validation()`: Verify `EnvironmentConfig` catches missing/invalid vars
- `test_agent_config_factory()`: Test OpenAI/Gemini client creation
- `test_embedding_dimensions()`: Ensure vectors are 384-dim
- `test_query_validation()`: Validate `UserQuery` content length and safety

### Integration Tests

- `test_qdrant_cloud_connection()`: Verify cloud cluster connectivity
- `test_vector_search()`: End-to-end search with real embeddings
- `test_agent_workflow()`: Query → search → agent → response pipeline
- `test_error_handling()`: Invalid credentials, network failures, timeouts

### Data Quality Tests

- `test_chunking_boundaries()`: Verify text splitter produces valid chunks
- `test_embedding_similarity()`: Sanity check semantic similarity scores
- `test_metadata_integrity()`: Ensure payload fields populated correctly

## Performance Characteristics

| Operation | Latency Target | Notes |
|-----------|----------------|-------|
| Env validation | <10ms | Startup only |
| Qdrant search | <500ms (p95) | Network latency + query execution |
| Embedding generation | <100ms/query | Local, no network calls |
| Agent response | <30s (p95) | LLM API call + processing |
| Indexing throughput | ≥10 docs/min | Batch upload to Qdrant Cloud |

## Security Considerations

- **Secrets Management**: All API keys in environment variables, never in code
- **TLS Encryption**: Qdrant Cloud uses HTTPS, Gemini/OpenAI use HTTPS
- **Input Validation**: Sanitize `UserQuery.content` to prevent injection attacks
- **Rate Limiting**: Implement per-user/session rate limits (future enhancement)
- **Audit Logging**: Log all API access with query IDs for traceability

## Future Extensibility

- **Multi-Collection Support**: Add collections for different textbook modules
- **User Authentication**: Integrate with auth system for personalized responses
- **Advanced Filtering**: Filter by textbook chapter, difficulty level, topic tags
- **Streaming Responses**: Support streaming for real-time agent output
- **Multi-Modal**: Add support for image/diagram embeddings (future SDK capability)
